\documentclass[../thesis.tex]{subfiles}

\begin{document}

% - The human brain is very complex -> neuroscientists proposing functional organization ->. functional selectivity 
% - What functional selectivity has been shown so far (talk in depth about FFA, then briefly mention other ones) 
% - Why does this matter (despite the controversy) 
% - We haven't focused much on food -> why looking at food is important and interesting (no unified visual component, involves semantic and visual characterization) 
% - What we do in this paper to focus on food (methods)
% - What are the potential impacts of food research

% Understanding the human brain is hard. Neuroscientists have investigated functional organization of the brain. 

% The human brain is arguably the most complex information processing system to exist. Understanding the workings h

% Understanding the workings of the human brain is arguably the most difficult t

The human brain is arguably the most complex information processing system to exist, and attempting to understanding it has remained a Herculean yet impactful feat. Emerging technology in industry has hoped to better understand the brain in efforts to create representative simulations and interpretable brain-machine interfaces \cite{statt_2019, musk2019integrated}. Biological fields are motivated to understand the brain in order to directly help patients with neurological disorders. Deep learning researchers benefit from findings about the brain to inspire novel model architectures, similar to how key components of Reinforcement Learning and Computer Vision have been inspired by discoveries of certain brain function characteristics \cite{lin1992self, fukushima1982neocognitron}. The impacts of better understanding the brain are clearly ample. \\

Significant research has furthered the understanding of the brain through investigation of its structural organization; this line of research has shown promising results. Studies have discovered the existence of various regions that tend to selectively respond strongly to specific categories. One notable region for example, the Fusiform Face Area (FFA), has been found to often respond selectively to the human face \cite{puce1995face}. In addition to the FFA, research has discovered the places, bodies, and visual words also correspond to specialized regions in the brain despite their high-level cognitive characteristics \cite{Kanwisher}. \\

Little research has considered the category of food as a potentially visually selective area past discovering certain areas activated by food. Food remains a fascinating category due to its lack of visual coherence across the category. In other words, there are little to no visual characteristics that remain uniform across the category of food, yet we are still able to recognize unseen foods as edible. This characteristic highlights the importance of the high-level processing to this potential region, and shines light on the fascinating aspect of a possible specialization despite such a diverse category. Specialization for this category goes beyond just food-related impact, and would emphasize that the brain has the ability to have a functional unit that is able to combine visual, semantic, and top-down influences to unify a diverse set of items. \\

In this work, we identify and investigate an apparent functional specialization for the food category in the brain. We do so by performing several machine learning and statistical methods on brain activation data to uncover consistent patterns that point to a specific cortical region's functional specialization. These techniques include encoding models that predict participant brain activity given the images that participants viewed, decoding models that predict viewed images' visual characteristics based on participant brain activity, and more. We use statistical methods to further break down the structure and functionality of this proposed food region. We then use state of the art deep learning models to confirm that patterns found in brain activity responses to food are not solely driven by low level visual features of the input images that participants viewed. \\

The identification of a food selective region has the potential to help inform how one would detect food in an environment, as well as how to organize food in a semantic system. It helps provide further insight into understanding how the brain may represent life-sustaining objects, and what kind of system we use to process food (whether it is ancient or not). Among many, some impacts on modern technology include improving food classifiers with the addition of brain response data, helping build reinforcement learning agents that seek out food, and identifying whether food identification could be a shared learning task vs a specialized task for robotics applications. 

\end{document}







%%%  Introduce for Autonomous Driving : Motivation %%%
% The rapid urbanization globally in the recent past has led to severe road congestion, rise in pollution levels and an increase in road accidents, therefore presenting a very grim picture of the current state of urban transportation. At the moment, private automobiles are widely recognized as an unsustainable solution for the future of personal urban mobility \cite{reinventing}. Fortunately however, great strides have been made in the development of autonomous driving technologies, energized by the successful demonstrations by some teams at the DARPA Urban Challenge \cite{boss, multimodaltartan}. While offering an opportunity to develop sustainable and safe solutions to personal mobility \cite{usecases_of_AD}, they also hint at a complete overhaul of the urban transportation landscape by ushering in Autonomous Vehicles-on-Demand. 

% %%%  Introduce for Autonomous Driving : Alternative learning-based approach %%%
% One of the key challenges to building accurate and robust autonomous navigation systems is to develop a strong intelligence pipeline that is able to efficiently gather incoming sensor data and take suitable control actions with good repeatability and fault-tolerance. In the past, this was addressed in a modular fashion, where  specialized algorithms were developed for each sub-system and later integrated with some fine tuning. More recently however, there is great interest in applying a more end-to-end approach wherein one can learn a complex mapping that goes directly from the input to the output by leveraging the availability to a large volume of task specific data. This approach is purported to perform better since we are optimizing the whole system. 
% % \todo{mention why this is better}

% %%% Introduce Deep Learning and Deep Reinforcement Learning : Motivation on true AI for decision making %%%%%%%%

% This end-to-end approach has now become a viable alternative thanks to the extension of deep learning's success to robotics and has been applied successfully to autonomous driving \cite{deepdriving,nvidiacar,endtoendcars}. 
% % in diverse domains like speech recognition\cite{graves2013speech,hinton2012deep}\todo{someone remove these?}, computer vision\cite{krizhevsky2012imagenet,simonyan2014very}, and 
% However, the traditional deep supervised learning-based driving requires labeling and may not be able to deal with the problem of accumulating errors \cite{ross2011reduction}. On the other hand, deep reinforcement learning (DRL) provides a much better formulation that allows policy improvement with feedback, and has been shown to achieve human-level performance on many gaming environments \cite{mnih2013playing, mnih2015human,2016-TOG-deepRL}.

% %%% Motivation of this work: Multi-modal Perception, Multi-modal Deep learning %%%%%%%%
% Previous work in DRL predominantly learned policies based on a single input modality, i.e., either low-dimensional physical states, or high-dimensional pixels. For autonomous driving task where enhancing safety and accuracy to the maximum extent possible is emphasized, developing policies that operate with multiple inputs is crucial. Indeed, multi-modal perception was an integral part of autonomous navigation solutions and even played a critical role in their success \cite{multimodaltartan} before the advent of end-to-end deep learning based approaches. Sensor fusion offers several advantages namely robustness to individual sensor noise/failure, improved object classification and tracking \cite{elfring2016multisensor, cho2014multi, darms2008classification}, robustness to varying weather and environmental conditions, etc. 
% % Previous work in DRL predominantly learned policies based on a single input modality, i.e., either low-dimensional physical states, or high-dimensional pixels. We have to however keep in mind the importance of enhancing safety and accuracy to the maximum extent possible in autonomous driving. In this light, we believe that it is worth exploring tangible ways to develop policies that operate with multiple inputs. 

% Recently, there is also an great progress on extending DRL approaches to multi-input fashion in order to tackle complex robotics tasks such as human-robot-interaction \cite{qureshi2016robot} and manipulation \cite{levine2016end}. Recently, \citet{mirowski2017a} proposed an novel approach namely \textit{NAV A3C} and embedded information such as vision, depth, and agent velocity for maze navigation. However, it is worth mentioned that the system dynamic is relatively simple compared with autonomous driving.
% % Multi-modal perception was an integral part of autonomous navigation solutions and even played a critical role in their success \cite{multimodaltartan} before the advent of end-to-end deep learning based approaches. Sensor fusion offers several advantages namely robustness to individual sensor noise/failure, improved object classification and tracking \cite{elfring2016multisensor, cho2014multi, darms2008classification}, robustness to varying weather and environmental conditions, etc. Multi-modal deep learning, in general, is an active area of research in other domains like audiovisual systems \cite{ngmultimodal}, gesture recognition \cite{moddrop}, text/speech and language models \cite{languagemultimodal,srivastava2012multimodal}, etc. However, Multi-modal learning is conspicuous by its absence in the modern end-to-end autonomous navigation literature. 

% %%% Brief summary of the contribution of this work  %%%%%%%%
% In this work, we present an end-to-end controller that uses multi-sensor input to learn an autonomous navigation policy in a physics-based gaming environment called TORCS \cite{wymann2000torcs}. To show the effectiveness of Multi-modal Perception, we picked two popular continuous action DRL algorithms namely Normalized Advantage Function (NAF) \cite{CDQN} and Deep Deterministic Policy Gradient (DDPG) \cite{DBLP:journals/corr/LillicrapHPHETS15}, and augmented them to accept multi-modal input. We limit our objective to only achieving autonomous navigation without any obstacles or other cars. This problem is kept simpler deliberately to focus our attention more on analyzing the performance of the proposed multi-modal configurations using extensive quantitative and qualitative testing. We believe however that it is representative of the larger urban navigation problem, and we ensure that our proposed network architecture is task-agnostic and transferable. We show through extensive empirical testing that a multi-modal deep reinforcement learning architecture achieves higher average reward.
% % \todo{add more details about results}

% Moreover, we also observe that while a multi-modal policy greatly improves the reward, it might rely heavily on all the inputs to the extent that it may fail completely even if a single sensor broke down fully or partially. This undesirable consequence renders sensor redundancy useless. To ensure that the learned policy does not succumb to such over-fitting, we apply a novel stochastic regularization method called \emph{Sensor Dropout} during training. 

% %%% Intro : Summarize different current approaches Dropout, DropConnect, ModDrop, Zoneout, Blockout, Modout, etc. %%%
% Stochastic regularization is an active area of research in deep learning made popular by the succes of, \textit{Dropout} \cite{dropout}. Following this landmark paper, numerous extensions were proposed  to further generalize this idea such as \textit{Blockout} \cite{blockout}, \textit{DropConnect} \cite{dropconnect}, \textit{Zoneout} \cite{zoneout}, etc. In the similar vein, two interesting techniques have been proposed for specialized regularization in the multi-modal setting namely ModDrop \cite{moddrop} and ModOut \cite{modout}. Given a much wider set of sensors to choose from, ModOut attempts to identify which sensors are actually needed to fully observe the system behavior. This is out of the scope of this work. Here, we assume that all the sensors are critical and we only focus on improving the state information based on inputs from multiple observers. ModDrop is much closer in spirit to the proposed \emph{Sensor Dropout (SD)}. However, unlike ModDrop, pretraining with individual sensor inputs using separate loss functions is not required. A network can be directly constructed in an end-to-end fashion and \emph{Sensor Dropout} can be directly applied at the sensor fusion layer just like Dropout. Its appeal lies in its simplicity during implementation and is designed to be applicable even to the DRL setting. As far as we know, this is the first attempt at applying stochastic regularization in a DRL setting. We show extensive simulation results to validate the net improvement in performance and robustness with Sensor Dropout. 

% Through extensive empirical testing we show the following exciting results in this paper:
% \begin{enumerate}
% \item Multimodal-DRL with Sensor Dropout (SD) reduces performance drop in a noisy environment from $\approx 30\%$ to just $5\%$, when compared to a baseline single sensor system.
% \item Policies learned using SD best leverage the multi-modal setting by greatly reducing over-dependence on any one sensing modality. Additionally, for each sensor it was observed that SD enforces sparsity and promotes each sensor to base the policy primarily on intuitive and salient features.
% \item A multi-modal policy with SD guarantees functionality even in a face a sensor failure. This is a huge plus and the best use-case for the need for redundancy in safety-critical application like autonomous navigation.
% \end{enumerate}
% % \todo{come back and carefully review this last para after the finishing the paper. Highlight some key results here to grab attention} 
% %%% Paper Summary %%%

% % The report is organized as follows. Section 2 summarizes the background and two DRL algorithms we extended in this work. Section 3 introduces Multi-modal network architecture, and propose a new stochastic regularization technique called \emph{Sensor Dropout}. The performance of Sensor Dropout is then validated in Section 4, with the problem setup in the TORCS simulator, and other testing results. In Section 5, we summarize our results and discuss key insights obtained through this exercise. Finally, Section 6 contains conclusions and ideas for future work. 


